[
  {
    "objectID": "graphs-1300.html",
    "href": "graphs-1300.html",
    "title": "Statistical Tables and Graphics",
    "section": "",
    "text": "Don’t worry about the code used to generate these tables and graphs. It uses a programming language called “R” that is widely used by statisticians, but there there is absolutely no expectation for you to deal with it in this course.\nConsider our data set we generated in class during the first week. In particular, we’re going to look at the lengths of your favorite songs.\n\nclass.data &lt;- read.csv(\"class-data.csv\")\nsort(class.data$length_of_favorite_song)\n\n [1] 134 134 148 166 170 173 174 180 180 181 185 189 215 228 229 233 237 237 260\n[20] 297 371 480\n\n\nAs presented, it’s kind of overwhelming. How can we visualize this data set so that it’s more understandable?\nFirst, let’s break it up into classes. Since the smallest length is 134 s and the largest is 480 s, we’ll split the interval [130, 490] up into 12 classes of class width 30. Then we can construct a frequency table. The entries are the frequencies of each class, i.e., the number of elements in the data set in that class.\n\nbreaks &lt;- seq(130, 490, by = 30)\nclasses &lt;- cut(class.data$length_of_favorite_song, breaks, right = FALSE)\nas.data.frame(table(classes))\n\n     classes Freq\n1  [130,160)    3\n2  [160,190)    9\n3  [190,220)    1\n4  [220,250)    5\n5  [250,280)    1\n6  [280,310)    1\n7  [310,340)    0\n8  [340,370)    0\n9  [370,400)    1\n10 [400,430)    0\n11 [430,460)    0\n12 [460,490)    1\n\n\nIn this case, the lower class limits are 130, 160, 190, …, 460, and the upper class limits are 159, 189, …, 489. (Note that in the table above, R confusingly prints 160, 190, etc as the upper endpoint of each class. But it’s using interval notation, and 159 is the largest integer in the interval [130, 160), 189 is the largest integer in [160, 190), etc.)\nThe class boundaries are the midpoints of the adjacent upper and lower class limits, so in this case, 159.5, 189.5, …, 459.5. Note in particular that we do not consider outer endpoints.\nThe class midpoints are the midpoints of each class, i.e., the average of the lower class limit and the upper class limit. So the first class has midpoint \\(\\frac{130 + 159}{2} = 144.5\\).\nNext, let’s look at a relative frequency distribution. Here, the entries in each row correspond to the proportion (frequency divided by sample size, possibly given as a percent) in each class.\n\nas.data.frame(prop.table(table(classes)))\n\n     classes       Freq\n1  [130,160) 0.13636364\n2  [160,190) 0.40909091\n3  [190,220) 0.04545455\n4  [220,250) 0.22727273\n5  [250,280) 0.04545455\n6  [280,310) 0.04545455\n7  [310,340) 0.00000000\n8  [340,370) 0.00000000\n9  [370,400) 0.04545455\n10 [400,430) 0.00000000\n11 [430,460) 0.00000000\n12 [460,490) 0.04545455\n\n\nNext, we have a cumulative frequency distribution, where entries in each row are the sums of the frequencies in all the classes up to that point.\n\nas.data.frame(cumsum(table(classes)))\n\n          cumsum(table(classes))\n[130,160)                      3\n[160,190)                     12\n[190,220)                     13\n[220,250)                     18\n[250,280)                     19\n[280,310)                     20\n[310,340)                     20\n[340,370)                     20\n[370,400)                     21\n[400,430)                     21\n[430,460)                     21\n[460,490)                     22\n\n\nFinally, we have the relative cumulative frequency distribution, which is exactly what it sounds like – the proportions of the cumulative frequencies. Note that the final entry will always be 100%.\n\nas.data.frame(cumsum(prop.table(table(classes))))\n\n          cumsum(prop.table(table(classes)))\n[130,160)                          0.1363636\n[160,190)                          0.5454545\n[190,220)                          0.5909091\n[220,250)                          0.8181818\n[250,280)                          0.8636364\n[280,310)                          0.9090909\n[310,340)                          0.9090909\n[340,370)                          0.9090909\n[370,400)                          0.9545455\n[400,430)                          0.9545455\n[430,460)                          0.9545455\n[460,490)                          1.0000000"
  },
  {
    "objectID": "graphs-1300.html#frequency-distributions",
    "href": "graphs-1300.html#frequency-distributions",
    "title": "Statistical Tables and Graphics",
    "section": "",
    "text": "Don’t worry about the code used to generate these tables and graphs. It uses a programming language called “R” that is widely used by statisticians, but there there is absolutely no expectation for you to deal with it in this course.\nConsider our data set we generated in class during the first week. In particular, we’re going to look at the lengths of your favorite songs.\n\nclass.data &lt;- read.csv(\"class-data.csv\")\nsort(class.data$length_of_favorite_song)\n\n [1] 134 134 148 166 170 173 174 180 180 181 185 189 215 228 229 233 237 237 260\n[20] 297 371 480\n\n\nAs presented, it’s kind of overwhelming. How can we visualize this data set so that it’s more understandable?\nFirst, let’s break it up into classes. Since the smallest length is 134 s and the largest is 480 s, we’ll split the interval [130, 490] up into 12 classes of class width 30. Then we can construct a frequency table. The entries are the frequencies of each class, i.e., the number of elements in the data set in that class.\n\nbreaks &lt;- seq(130, 490, by = 30)\nclasses &lt;- cut(class.data$length_of_favorite_song, breaks, right = FALSE)\nas.data.frame(table(classes))\n\n     classes Freq\n1  [130,160)    3\n2  [160,190)    9\n3  [190,220)    1\n4  [220,250)    5\n5  [250,280)    1\n6  [280,310)    1\n7  [310,340)    0\n8  [340,370)    0\n9  [370,400)    1\n10 [400,430)    0\n11 [430,460)    0\n12 [460,490)    1\n\n\nIn this case, the lower class limits are 130, 160, 190, …, 460, and the upper class limits are 159, 189, …, 489. (Note that in the table above, R confusingly prints 160, 190, etc as the upper endpoint of each class. But it’s using interval notation, and 159 is the largest integer in the interval [130, 160), 189 is the largest integer in [160, 190), etc.)\nThe class boundaries are the midpoints of the adjacent upper and lower class limits, so in this case, 159.5, 189.5, …, 459.5. Note in particular that we do not consider outer endpoints.\nThe class midpoints are the midpoints of each class, i.e., the average of the lower class limit and the upper class limit. So the first class has midpoint \\(\\frac{130 + 159}{2} = 144.5\\).\nNext, let’s look at a relative frequency distribution. Here, the entries in each row correspond to the proportion (frequency divided by sample size, possibly given as a percent) in each class.\n\nas.data.frame(prop.table(table(classes)))\n\n     classes       Freq\n1  [130,160) 0.13636364\n2  [160,190) 0.40909091\n3  [190,220) 0.04545455\n4  [220,250) 0.22727273\n5  [250,280) 0.04545455\n6  [280,310) 0.04545455\n7  [310,340) 0.00000000\n8  [340,370) 0.00000000\n9  [370,400) 0.04545455\n10 [400,430) 0.00000000\n11 [430,460) 0.00000000\n12 [460,490) 0.04545455\n\n\nNext, we have a cumulative frequency distribution, where entries in each row are the sums of the frequencies in all the classes up to that point.\n\nas.data.frame(cumsum(table(classes)))\n\n          cumsum(table(classes))\n[130,160)                      3\n[160,190)                     12\n[190,220)                     13\n[220,250)                     18\n[250,280)                     19\n[280,310)                     20\n[310,340)                     20\n[340,370)                     20\n[370,400)                     21\n[400,430)                     21\n[430,460)                     21\n[460,490)                     22\n\n\nFinally, we have the relative cumulative frequency distribution, which is exactly what it sounds like – the proportions of the cumulative frequencies. Note that the final entry will always be 100%.\n\nas.data.frame(cumsum(prop.table(table(classes))))\n\n          cumsum(prop.table(table(classes)))\n[130,160)                          0.1363636\n[160,190)                          0.5454545\n[190,220)                          0.5909091\n[220,250)                          0.8181818\n[250,280)                          0.8636364\n[280,310)                          0.9090909\n[310,340)                          0.9090909\n[340,370)                          0.9090909\n[370,400)                          0.9545455\n[400,430)                          0.9545455\n[430,460)                          0.9545455\n[460,490)                          1.0000000"
  },
  {
    "objectID": "graphs-1300.html#histograms",
    "href": "graphs-1300.html#histograms",
    "title": "Statistical Tables and Graphics",
    "section": "Histograms",
    "text": "Histograms\nA nice visual representation of a frequency distribution is a histogram. Along the horizontal axis, we have the classes. And along the vertical axis, we have the frequencies. Over each class, we draw a bar whose height is the corresponding frequency.\n\nhist(class.data$length_of_favorite_song, breaks, right = FALSE,\n     xlab = \"Favorite song lengths\",\n     main = \"Histogram of favorite song lengths\")\n\n\n\n\n\n\n\n\nThis data set is skewed right. See the examples on p. 41 of the textbook for a few other possibilities."
  },
  {
    "objectID": "graphs-1300.html#other-statistical-graphics",
    "href": "graphs-1300.html#other-statistical-graphics",
    "title": "Statistical Tables and Graphics",
    "section": "Other Statistical Graphics",
    "text": "Other Statistical Graphics\nThere are a wide variety of statistical graphs touched on in Section 3.3 of our textbook. We’ll focus on two of them (which are the ones you’ll encounter in Homework 3).\n\nStem plots\nThese are kind of like a sideways histogram where we use numbers instead of bars. Each row has 10’s place number as its heading and the rest of the entries are the numbers from the 1’s place. Naturally, it works best with 2-digit numbers.\nLet’s play with our data set a bit to turn it into two digit-numbers. In particular, we’ll divide by 10 and then round. So the new data set is of song lengths in dekaseconds (1 dekasecond = 10 seconds).\n\nsong.length.in.dekaseconds &lt;- round(class.data$length_of_favorite_song / 10)\nsort(song.length.in.dekaseconds)\n\n [1] 13 13 15 17 17 17 17 18 18 18 18 19 22 23 23 23 24 24 26 30 37 48\n\n\nNow let’s see a stem plot:\n\nstem(song.length.in.dekaseconds)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  1 | 335777788889\n  2 | 2333446\n  3 | 07\n  4 | 8\n\n\n\n\nScatter plots\nIf we have paired numerical data, then we can plot points in the plane, making one variable the \\(x\\)-coordinate and the other variable the \\(y\\)-coordinate. This gives us a scatter plot.\nLet’s plot favorite song length against number of siblings.\n\nplot(class.data$length_of_favorite_song, class.data$number_of_siblings,\n     xlab = \"Length of favorite song\",\n     ylab = \"Number of siblings\")\n\n\n\n\n\n\n\n\n\n\nTime series graphs\nIf we connect the dots in a scatter plot, we get a line graph. And if the \\(x\\)-coordinates correspond to a particular time, then we get a time-series graph.\nWe don’t really have any good data from our data set for a time series graph, so here’s one that’s built into R. The AirPassengers gives the monthly totals (in thousands) of international airline passengers from 1949 to 1960.\n\nplot(AirPassengers)"
  },
  {
    "objectID": "jan15.html",
    "href": "jan15.html",
    "title": "Monte Carlo simulations for the beta distribution",
    "section": "",
    "text": "Recall that if \\(X\\) has the beta distribution with shape parameters \\(\\alpha\\) and \\(\\beta\\), i.e., \\(X\\sim\\operatorname{Beta}(\\alpha,\\beta)\\), then \\(X\\) has probability density function\n\\[f_X(x) = \\begin{cases}\n\\frac{1}{B(\\alpha)(\\beta)}x^{\\alpha - 1}(1 - x)^{\\beta - 1} & \\text{if }0 &lt; x &lt; 1 \\\\\n0 & \\text{otherwise,}\n\\end{cases}\n\\] where \\(B\\) is the beta function given by \\[B(\\alpha,\\beta) = \\int_0^1x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\,dx = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}.\\]\nWe saw in class that if \\(X_1,X_2\\sim\\operatorname{Beta}(2, 1)\\), then their sample mean \\(\\overline X\\) has cumulative distribution function \\[\nF_{\\overline X}(z) = \\begin{cases}\n0 &\\text{if }z \\leq 0 \\\\\n\\frac{8}{3}z^4 &\\text{if } 0 &lt; z \\leq\\frac{1}{2} \\\\\n-\\frac{8}{3}z^4 + 8z^2 - \\frac{16}{3}z + 1 &\\text{if }\\frac{1}{2} &lt; z &lt; 1 \\\\\n1 &\\text{if }z \\geq 1\\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "jan15.html#beta-distribution",
    "href": "jan15.html#beta-distribution",
    "title": "Monte Carlo simulations for the beta distribution",
    "section": "",
    "text": "Recall that if \\(X\\) has the beta distribution with shape parameters \\(\\alpha\\) and \\(\\beta\\), i.e., \\(X\\sim\\operatorname{Beta}(\\alpha,\\beta)\\), then \\(X\\) has probability density function\n\\[f_X(x) = \\begin{cases}\n\\frac{1}{B(\\alpha)(\\beta)}x^{\\alpha - 1}(1 - x)^{\\beta - 1} & \\text{if }0 &lt; x &lt; 1 \\\\\n0 & \\text{otherwise,}\n\\end{cases}\n\\] where \\(B\\) is the beta function given by \\[B(\\alpha,\\beta) = \\int_0^1x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\,dx = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}.\\]\nWe saw in class that if \\(X_1,X_2\\sim\\operatorname{Beta}(2, 1)\\), then their sample mean \\(\\overline X\\) has cumulative distribution function \\[\nF_{\\overline X}(z) = \\begin{cases}\n0 &\\text{if }z \\leq 0 \\\\\n\\frac{8}{3}z^4 &\\text{if } 0 &lt; z \\leq\\frac{1}{2} \\\\\n-\\frac{8}{3}z^4 + 8z^2 - \\frac{16}{3}z + 1 &\\text{if }\\frac{1}{2} &lt; z &lt; 1 \\\\\n1 &\\text{if }z \\geq 1\\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "jan15.html#monte-carlo-simulation",
    "href": "jan15.html#monte-carlo-simulation",
    "title": "Monte Carlo simulations for the beta distribution",
    "section": "Monte Carlo simulation",
    "text": "Monte Carlo simulation\nFirst, let’s define an R function to evaluate our cdf from above:\n\nF &lt;- function(z) {\n  if (z &lt;= 0) {\n    return(0)\n  } else if (z &lt;= 1/2) {\n    return(8/3 * z^4)\n  } else if (z &lt; 1 ) {\n    return (-8/3 * z^4 + 8*z^2 - 16/3*z + 1)\n  } else {\n    return(1)\n  }\n}\n\nNext, we can use R to generate a random sample of \\(\\overline X\\) by first using rbeta(2, 2, 1) to generate 2 random samples from a \\(\\operatorname{Beta}(2, 1)\\) distribution, and then mean to find their sample mean.\n\nmean(rbeta(2,2,1))\n\n[1] 0.7937156\n\n\nTo perform our Monte Carlo simulation, we use replicate to perform the process a bunch of times. Each time, we compare to some value, yielding TRUE or FALSE, and then mean to find the proportion of TRUE’s. (Since TRUE is treated like 1 and FALSE like 0.)\nLet’s choose several different choices for \\(z\\) and compare the theoretical results with our simulated results.\n\nz &lt;- 0.25\nF(z)\n\n[1] 0.01041667\n\nmean(replicate(10000, mean(rbeta(2,2,1)) &lt; z))\n\n[1] 0.009\n\n\n\nz &lt;- 0.5\nF(z)\n\n[1] 0.1666667\n\nmean(replicate(10000, mean(rbeta(2,2,1)) &lt; z))\n\n[1] 0.1609\n\n\n\nz &lt;- 0.75\nF(z)\n\n[1] 0.65625\n\nmean(replicate(10000, mean(rbeta(2,2,1)) &lt; z))\n\n[1] 0.6573\n\n\nNot bad!"
  },
  {
    "objectID": "test1.html",
    "href": "test1.html",
    "title": "Test 1 Monte Carlo simulations",
    "section": "",
    "text": "Let’s check for \\(n\\in\\{2,\\ldots,10\\}\\).\n\nc(1/2^2, mean(replicate(10000, max(rbinom(2, 1, 1/2)) == 0)))\n\n[1] 0.2500 0.2546\n\nc(1/2^3, mean(replicate(10000, max(rbinom(3, 1, 1/2)) == 0)))\n\n[1] 0.125 0.125\n\nc(1/2^4, mean(replicate(10000, max(rbinom(4, 1, 1/2)) == 0)))\n\n[1] 0.0625 0.0609\n\nc(1/2^5, mean(replicate(10000, max(rbinom(5, 1, 1/2)) == 0)))\n\n[1] 0.03125 0.03130\n\nc(1/2^6, mean(replicate(10000, max(rbinom(6, 1, 1/2)) == 0)))\n\n[1] 0.015625 0.015300\n\nc(1/2^7, mean(replicate(10000, max(rbinom(7, 1, 1/2)) == 0)))\n\n[1] 0.0078125 0.0080000\n\nc(1/2^8, mean(replicate(10000, max(rbinom(8, 1, 1/2)) == 0)))\n\n[1] 0.00390625 0.00290000\n\nc(1/2^9, mean(replicate(10000, max(rbinom(9, 1, 1/2)) == 0)))\n\n[1] 0.001953125 0.001300000\n\nc(1/2^10, mean(replicate(10000, max(rbinom(10, 1, 1/2)) == 0)))\n\n[1] 0.0009765625 0.0005000000"
  },
  {
    "objectID": "test1.html#problem-1",
    "href": "test1.html#problem-1",
    "title": "Test 1 Monte Carlo simulations",
    "section": "",
    "text": "Let’s check for \\(n\\in\\{2,\\ldots,10\\}\\).\n\nc(1/2^2, mean(replicate(10000, max(rbinom(2, 1, 1/2)) == 0)))\n\n[1] 0.2500 0.2546\n\nc(1/2^3, mean(replicate(10000, max(rbinom(3, 1, 1/2)) == 0)))\n\n[1] 0.125 0.125\n\nc(1/2^4, mean(replicate(10000, max(rbinom(4, 1, 1/2)) == 0)))\n\n[1] 0.0625 0.0609\n\nc(1/2^5, mean(replicate(10000, max(rbinom(5, 1, 1/2)) == 0)))\n\n[1] 0.03125 0.03130\n\nc(1/2^6, mean(replicate(10000, max(rbinom(6, 1, 1/2)) == 0)))\n\n[1] 0.015625 0.015300\n\nc(1/2^7, mean(replicate(10000, max(rbinom(7, 1, 1/2)) == 0)))\n\n[1] 0.0078125 0.0080000\n\nc(1/2^8, mean(replicate(10000, max(rbinom(8, 1, 1/2)) == 0)))\n\n[1] 0.00390625 0.00290000\n\nc(1/2^9, mean(replicate(10000, max(rbinom(9, 1, 1/2)) == 0)))\n\n[1] 0.001953125 0.001300000\n\nc(1/2^10, mean(replicate(10000, max(rbinom(10, 1, 1/2)) == 0)))\n\n[1] 0.0009765625 0.0005000000"
  },
  {
    "objectID": "test1.html#problem-2",
    "href": "test1.html#problem-2",
    "title": "Test 1 Monte Carlo simulations",
    "section": "Problem 2",
    "text": "Problem 2\n\nmean(replicate(10000, mean(rbinom(2, 2, 1/4)) == 1))\n\n[1] 0.2034"
  },
  {
    "objectID": "test1.html#problem-3",
    "href": "test1.html#problem-3",
    "title": "Test 1 Monte Carlo simulations",
    "section": "Problem 3",
    "text": "Problem 3\n\nmean(replicate(10000, var(runif(2)) &lt; 1/8))\n\n[1] 0.75"
  },
  {
    "objectID": "test1.html#problem-4",
    "href": "test1.html#problem-4",
    "title": "Test 1 Monte Carlo simulations",
    "section": "Problem 4",
    "text": "Problem 4\n\nmean(replicate(10000, mean(rnorm(20)^2) &gt; 0.47955))\n\n[1] 0.9761"
  },
  {
    "objectID": "test1.html#problem-5",
    "href": "test1.html#problem-5",
    "title": "Test 1 Monte Carlo simulations",
    "section": "Problem 5",
    "text": "Problem 5\nSince we don’t know \\(\\sigma^2\\), let’s try a few different values and ensure we get the same thing.\n\nmean(replicate(10000, {x &lt;- rnorm(16); mean(x) &gt; 0.93325 * sd(x)}))\n\n[1] 9e-04\n\nmean(replicate(10000, {x &lt;- rnorm(16, 0, 4); mean(x) &gt; 0.93325 * sd(x)}))\n\n[1] 9e-04\n\nmean(replicate(10000, {x &lt;- rnorm(16, 0, 9); mean(x) &gt; 0.93325 * sd(x)}))\n\n[1] 0.0011\n\nmean(replicate(10000, {x &lt;- rnorm(16, 0, 16); mean(x) &gt; 0.93325 * sd(x)}))\n\n[1] 5e-04\n\nmean(replicate(10000, {x &lt;- rnorm(16, 0, 25); mean(x) &gt; 0.93325 * sd(x)}))\n\n[1] 7e-04"
  },
  {
    "objectID": "feb10.html",
    "href": "feb10.html",
    "title": "Maximum Likelihood Estimators",
    "section": "",
    "text": "Recall from class that we found that the maximum likelihood estimator for the rate parameter \\(\\lambda\\) in the exponential distribution \\(\\operatorname{Exp}(\\lambda)\\) is \\(\\hat\\lambda = \\frac{1}{\\overline X}\\).\nLet’s see how we did. We’ll generate 1000 random samples from an \\(\\operatorname{Exp}{(54)}\\) distribution to see how we did.\n\nx &lt;- rexp(1000, 54)\nlambdaHat &lt;- 1/mean(x)\nlambdaHat\n\n[1] 53.56418\n\n\nLet’s also examine it graphically by plotting:\n\nthe “empiricial cumulative distribution function”, which gives us the proportion of the data in our data set bounded above by the given value. The plot.ecdf function gives us this.\nthe cumulative distribution function of an exponential distribution with our estimate \\(\\hat\\lambda\\) as its rate parameter. We first create two vectors, xx (which contains all values between 0 and 0.15, going up by 0.01) and yy (which we get by plugging all the elements of xx into the cdf of our estimated distribution), and plot them using the lines function.\n\n\nplot.ecdf(x)\nxx &lt;- seq(0, 0.15, 0.01)\nyy &lt;- pexp(xx, lambdaHat)\nlines(xx, yy, col=\"red\")\n\n\n\n\n\n\n\n\nNow let’s do the same thing for our other two examples."
  },
  {
    "objectID": "feb10.html#exponential-distribution",
    "href": "feb10.html#exponential-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "",
    "text": "Recall from class that we found that the maximum likelihood estimator for the rate parameter \\(\\lambda\\) in the exponential distribution \\(\\operatorname{Exp}(\\lambda)\\) is \\(\\hat\\lambda = \\frac{1}{\\overline X}\\).\nLet’s see how we did. We’ll generate 1000 random samples from an \\(\\operatorname{Exp}{(54)}\\) distribution to see how we did.\n\nx &lt;- rexp(1000, 54)\nlambdaHat &lt;- 1/mean(x)\nlambdaHat\n\n[1] 53.56418\n\n\nLet’s also examine it graphically by plotting:\n\nthe “empiricial cumulative distribution function”, which gives us the proportion of the data in our data set bounded above by the given value. The plot.ecdf function gives us this.\nthe cumulative distribution function of an exponential distribution with our estimate \\(\\hat\\lambda\\) as its rate parameter. We first create two vectors, xx (which contains all values between 0 and 0.15, going up by 0.01) and yy (which we get by plugging all the elements of xx into the cdf of our estimated distribution), and plot them using the lines function.\n\n\nplot.ecdf(x)\nxx &lt;- seq(0, 0.15, 0.01)\nyy &lt;- pexp(xx, lambdaHat)\nlines(xx, yy, col=\"red\")\n\n\n\n\n\n\n\n\nNow let’s do the same thing for our other two examples."
  },
  {
    "objectID": "feb10.html#poisson-distribution",
    "href": "feb10.html#poisson-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\nx &lt;- rpois(1000, 54)\nlambdaHat &lt;- mean(x)\nlambdaHat\n\n[1] 54.464\n\nplot.ecdf(x)\nxx &lt;- seq(30, 80)\nyy &lt;- ppois(xx, 54)\nlines(xx, yy, col=\"blue\")"
  },
  {
    "objectID": "feb10.html#normal-distribution",
    "href": "feb10.html#normal-distribution",
    "title": "Maximum Likelihood Estimators",
    "section": "Normal distribution",
    "text": "Normal distribution\n\nx &lt;- rnorm(1000, 5, 4)\nmuHat &lt;- mean(x)\nsigmasqHat &lt;- 999/1000 * var(x)\nc(muHat, sigmasqHat)\n\n[1]  5.097825 15.863403\n\n\nNote that the third argument to rnorm specifies the standard deviation \\(\\sigma\\) and not the variance \\(\\sigma^2\\), so we got an estimate of \\(4^2 = 16\\).\n\nplot.ecdf(x)\nxx &lt;- seq(-10, 20)\nyy &lt;- pnorm(xx, 5, 4)\nlines(xx, yy, col=\"green\")"
  },
  {
    "objectID": "jan20.html",
    "href": "jan20.html",
    "title": "Variance of Dice",
    "section": "",
    "text": "First, note that we may use the sample data from an arbitrary discrete distribution with relatively small support. Read help(sample) for more, but the basic gist is that we give it the support x, the size (number of samples), a boolean replace which tells us if we sample with replacement or not (we will almost always use TRUE here since we want our samples to be independent), and prob is essentially the probability mass function. In particular, the first element of prob is the probability of the first element of x, the second element of prob is the probability of the second element of x and so on. We construct x and prob using the “combine” function c. Note that we may omit prob if the distribution is uniform. In other words, the following are equivalent:\nsample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE, c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\nsample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)\nWe used the longer version in class, but we’ll use the shorter version in this document."
  },
  {
    "objectID": "jan20.html#sampling",
    "href": "jan20.html#sampling",
    "title": "Variance of Dice",
    "section": "",
    "text": "First, note that we may use the sample data from an arbitrary discrete distribution with relatively small support. Read help(sample) for more, but the basic gist is that we give it the support x, the size (number of samples), a boolean replace which tells us if we sample with replacement or not (we will almost always use TRUE here since we want our samples to be independent), and prob is essentially the probability mass function. In particular, the first element of prob is the probability of the first element of x, the second element of prob is the probability of the second element of x and so on. We construct x and prob using the “combine” function c. Note that we may omit prob if the distribution is uniform. In other words, the following are equivalent:\nsample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE, c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\nsample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)\nWe used the longer version in class, but we’ll use the shorter version in this document."
  },
  {
    "objectID": "jan20.html#sample-variance-of-2d6",
    "href": "jan20.html#sample-variance-of-2d6",
    "title": "Variance of Dice",
    "section": "Sample Variance of 2d6",
    "text": "Sample Variance of 2d6\nRecall our goal: to find the sample variance \\(S^2\\) of 2d6, i.e, of i.i.d. random variables \\(X_1,X_2\\) with the uniform distribution on \\(\\{1,2,3,4,5,6\\}\\).\nIn class, we found that the probability mass function of \\(S^2\\) is \\[\nf_{S^2}(x) = \\begin{cases}\n\\frac{1}{6} &\\text{if }x = 0 \\\\\n\\frac{5}{18} &\\text{if }x = \\frac{1}{2} \\\\\n\\frac{2}{9} &\\text{if }x = 2 \\\\\n\\frac{1}{6} &\\text{if }x = \\frac{9}{2} \\\\\n\\frac{1}{9} &\\text{if }x = 8 \\\\\n\\frac{1}{18} &\\text{if }x = \\frac{25}{2} \\\\\n0 &\\text{otherwise.}\n\\end{cases}\n\\]\nand that \\(E(S^2) = \\frac{35}{12}\\), which happens to be equal to \\(\\operatorname{Var}(X_i)\\)."
  },
  {
    "objectID": "jan20.html#monte-carlo-simulations",
    "href": "jan20.html#monte-carlo-simulations",
    "title": "Variance of Dice",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nFirst, let’s check the expected value. Recall that to perform a Monte Carlo simulation, we use mean(replicate(...)) to run our code a bunch of times and find the average value.\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE))))\n\n[1] 2.90875\n\n35/12\n\n[1] 2.916667\n\n\nNow let’s check the values of our probability mass function. Recall that by have a statement (e.g., var(...) == 0), replicate(...) spits out a bunch of booleans (TRUE and FALSE), and mean(...) gives us the proportion of TRUE’s. This gives us an estimate of the probability of that particular value.\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 0))\n\n[1] 0.1705\n\n1/6\n\n[1] 0.1666667\n\n\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 1/2))\n\n[1] 0.2818\n\n5/18\n\n[1] 0.2777778\n\n\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 2))\n\n[1] 0.2208\n\n2/9\n\n[1] 0.2222222\n\n\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 9/2))\n\n[1] 0.1675\n\n1/6\n\n[1] 0.1666667\n\n\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 8))\n\n[1] 0.1116\n\n1/9\n\n[1] 0.1111111\n\n\n\nmean(replicate(10000, var(sample(c(1, 2, 3, 4, 5, 6), 2, replace = TRUE)) == 25/2))\n\n[1] 0.0512\n\n1/18\n\n[1] 0.05555556"
  },
  {
    "objectID": "jan20.html#binomial-distribution",
    "href": "jan20.html#binomial-distribution",
    "title": "Variance of Dice",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nWe also started a second problem. Suppose \\(X_1,X_2\\sim B\\left(3, \\frac{1}{2}\\right)\\). We computed\n\\[\\begin{align}\nP(S^2 = 0) &= \\sum_{k=0}^3 P(X_1 = k)P(X_2=k) \\\\\n&= \\sum_{k=0}^3\\left(\\binom{3}{k}\\left(\\frac{1}{2}\\right)^k\\left(\\frac{1}{2}\\right)^{3-k}\\right)^2 \\\\\n&= \\sum_{k=0}\\binom{3}{k}^2\\frac{1}{64}\\\\\n&= \\frac{1}{64}\\left(1^2 + 3^2 + 3^2 + 1^3\\right) \\\\\n&= \\frac{5}{16}\n\\end{align}\\]\nLet’s check this with a Monte Carlo simulation as well. We may use the rbinom function. It takes three arguments: the number of samples and then the \\(n\\) and \\(p\\) from our binomial distribution. So we want to use rbinom(2, 3, 1/2).\n\nmean(replicate(10000, var(rbinom(2, 3, 1/2)) == 0))\n\n[1] 0.3183\n\n5/16\n\n[1] 0.3125"
  },
  {
    "objectID": "jan13.html",
    "href": "jan13.html",
    "title": "Monte Carlo Simulations",
    "section": "",
    "text": "Recall that we found that if \\(X_1, X_2\\sim U(0, 1)\\), then the minimum \\(X_{(1)}\\) has probability density function \\(f_{X_{(1)}}(x) = 2(1-x)\\). Integrating, its cumulative distribution function (on the unit interval) is \\[\\begin{align}\nF_{X_{(1)}}(x) &= \\int_0^x 2(1 - t)\\,dt \\\\\n&= -(1-t)^2\\Big|_0^x \\\\\n&= -(1-x)^2 + (1 - 0)^2 \\\\\n&= -1 + 2x - x^2 + 1 \\\\\n&= -x^2 + 2x\n\\end{align}\\]\nLet’s check this with some Monte Carlo simulations. For example, we should expect that the probability that our minimum is less than \\(\\frac{1}{2}\\) is\n\\[F_{X_{(1)}}(0.5) = -0.5^2 + 2\\cdot 0.5 = 0.75\\]\nRecall that runif(2) will generate a random sample from our, and min(runif(2)) will give us the minimum. We can do this multiple times with replicate:\n\nreplicate(10, min(runif(2)))\n\n [1] 0.19860485 0.35366928 0.85832971 0.64318600 0.73555678 0.57205241\n [7] 0.26672812 0.25328451 0.58655646 0.06290076\n\n\nWe’re interested in finding the number of our random samples that were less than 0.5. If we replicate a statement, we’ll get a list of a bunch of boolean (TRUE or FALSE) objects.\n\nreplicate(10, min(runif(2)) &lt; 0.5)\n\n [1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n\n\nR treats these Boolean objects like integers (0 for FALSE and 1 for TRUE), so we can compute the proportion of TRUE’s by taking the mean:\n\nmean(replicate(10, min(runif(2)) &lt; 0.5))\n\n[1] 0.8\n\n\nNow let’s increase the sample size:\n\nmean(replicate(10000, min(runif(2)) &lt; 0.5))\n\n[1] 0.745\n\n\nWe got pretty close to our theoretical result of 0.75!"
  },
  {
    "objectID": "jan8.html",
    "href": "jan8.html",
    "title": "Statistical Graphics",
    "section": "",
    "text": "Statistical Graphics\nWe will use the software platform R to do many computations.\nR comes with various builtin datasets to play around with.\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nThere are various ways to represent data visually using graphics. One example is a “mosaic plot”:\n\nmosaicplot(~ Eye + Sex, data=HairEyeColor)\n\n\n\n\n\n\n\n\nVisually, it appears that hair and eye color may be independent from sex, but eye and hair color may be dependent on each other.\nWe can import data sets using the scan function:\n\nspeed.of.light &lt;- scan(\"michelson1.d\")\nsort(speed.of.light)\n\n  [1]  620  650  720  720  720  740  740  740  750  760  760  760  760  760  770\n [16]  780  780  790  790  790  800  800  800  800  800  810  810  810  810  810\n [31]  810  810  810  810  810  820  820  830  830  840  840  840  840  840  840\n [46]  840  840  850  850  850  850  850  850  850  850  860  860  860  870  870\n [61]  870  870  880  880  880  880  880  880  880  880  880  880  890  890  890\n [76]  900  900  910  910  920  930  930  940  940  940  950  950  950  960  960\n [91]  960  960  970  980  980  980 1000 1000 1000 1070\n\nstem(speed.of.light)\n\n\n  The decimal point is 2 digit(s) to the right of the |\n\n   6 | 2\n   6 | 5\n   7 | 222444\n   7 | 566666788999\n   8 | 000001111111111223344444444\n   8 | 5555555566677778888888888999\n   9 | 0011233444\n   9 | 55566667888\n  10 | 000\n  10 | 7\n\n\n\nhist(speed.of.light)\n\n\n\n\n\n\n\n\nBased on these graphics, it appears that these measurements may come from a normal distribution.\n\nboxplot(speed.of.light)\n\n\n\n\n\n\n\n\n\nquantile(speed.of.light, c(0, 0.25, 0.5, 0.75, 1))\n\n    0%    25%    50%    75%   100% \n 620.0  807.5  850.0  892.5 1070.0 \n\n\n\nplot.ecdf(speed.of.light, ylab=\"F_n(x)\")\n\n\n\n\n\n\n\n\n\nhelp(plot.ecdf)"
  },
  {
    "objectID": "feb5.html",
    "href": "feb5.html",
    "title": "Method of Moments",
    "section": "",
    "text": "Let’s suppose that \\(X_i\\sim B(20, 0.67)\\). Our goal is to estimate \\(n=20\\) and \\(p = 0.67\\).\nWe found in class the method of moments estimators \\(\\hat p = 1 - \\frac{M_2 - M_1^2}{M1}\\) and \\(\\hat n = \\frac{M_1}{\\hat p}\\). Now let’s generate a bunch of random binomially distributed data with the parameters that we secretly know and see how well we did.\n\nx &lt;- rbinom(1000, 20, 0.67)\nM1 &lt;- mean(x)\nM2 &lt;- mean(x^2)\npHat &lt;- 1 - (M2 - M1^2)/M1\nnHat &lt;- M1 / pHat\nc(nHat, pHat)\n\n[1] 20.0461216  0.6712022\n\n\nNow suppose \\(X_i\\sim U(2, 28)\\). Our goal is to estimate \\(a = 2\\) and \\(b = 28\\).\nWe found in class the method of moments estimators \\(\\hat a = M_1 - \\sqrt{3M_2 - 3M_1^2}\\) and \\(\\hat b = M_1 + \\sqrt{3M_2 - 3M_1^2}\\). Let see how we did:\n\nx &lt;- runif(1000, 2, 28)\nM1 &lt;- mean(x)\nM2 &lt;- mean(x^2)\naHat &lt;- M1 - sqrt(3*M2 - 3*M1^2)\nbHat &lt;- M1 + sqrt(3*M2 - 3*M1^2)\nc(aHat, bHat)\n\n[1]  2.00946 27.74034"
  },
  {
    "objectID": "jan22.html",
    "href": "jan22.html",
    "title": "Computing various probabiltiies in R",
    "section": "",
    "text": "Suppose \\(Z_1,\\ldots,Z_5\\sim N(0,1)\\) are independent random variables. Let’s find the probability that \\(Z_1^2 + \\cdots + Z_5^2\\) is at least 3, i.e., \\(P(Z_1^2 + \\cdots + Z_5^2 &gt; 3)\\). This sum has distribution \\(\\chi^2(5)\\). So if \\(W\\sim \\chi^2(5)\\), we can use \\(P(W &gt; 3) = 1 - P(W \\leq 3)\\).\nIn R, if we want to compute values of the cumulative distribution function of a particular distribution, we use a function beginning with p. In the case of the chi-squared distribution, that function is pchiqsq. In general if \\(W\\sim\\chi^2(\\nu)\\) and we want to compute \\(P(W &lt; w)\\), we use pchiqsq(w, ν).\nSo in this case, we have:\n\n1 - pchisq(3, 5)\n\n[1] 0.6999858\n\n\nAlternatively, we can skip the 1 - and use the lower.tail = FALSE option to switch the direction of the inequality.\n\npchisq(3, 5, lower.tail = FALSE)\n\n[1] 0.6999858"
  },
  {
    "objectID": "jan22.html#chi-squared-distribution",
    "href": "jan22.html#chi-squared-distribution",
    "title": "Computing various probabiltiies in R",
    "section": "",
    "text": "Suppose \\(Z_1,\\ldots,Z_5\\sim N(0,1)\\) are independent random variables. Let’s find the probability that \\(Z_1^2 + \\cdots + Z_5^2\\) is at least 3, i.e., \\(P(Z_1^2 + \\cdots + Z_5^2 &gt; 3)\\). This sum has distribution \\(\\chi^2(5)\\). So if \\(W\\sim \\chi^2(5)\\), we can use \\(P(W &gt; 3) = 1 - P(W \\leq 3)\\).\nIn R, if we want to compute values of the cumulative distribution function of a particular distribution, we use a function beginning with p. In the case of the chi-squared distribution, that function is pchiqsq. In general if \\(W\\sim\\chi^2(\\nu)\\) and we want to compute \\(P(W &lt; w)\\), we use pchiqsq(w, ν).\nSo in this case, we have:\n\n1 - pchisq(3, 5)\n\n[1] 0.6999858\n\n\nAlternatively, we can skip the 1 - and use the lower.tail = FALSE option to switch the direction of the inequality.\n\npchisq(3, 5, lower.tail = FALSE)\n\n[1] 0.6999858"
  },
  {
    "objectID": "jan22.html#normal-distribution",
    "href": "jan22.html#normal-distribution",
    "title": "Computing various probabiltiies in R",
    "section": "Normal distribution",
    "text": "Normal distribution\nLet’s say \\(X_1,\\ldots,X_{10}\\sim N(1, 25)\\) are independent. (Note that this means \\(\\sigma^2 = 25\\) and so \\(\\sigma = 5\\).) Find the probability that \\(\\overline X\\) is less than 2.\nWe know that \\(\\dfrac{\\overline X - \\mu}{\\sigma/\\sqrt {n}}\\sim N(0,1)\\), and so if we say that \\(Z\\sim N(0,1)\\), then we have:\n\\[P(\\overline X &lt; 2) = P\\left(Z &lt; \\frac{2 - 1}{5/\\sqrt{10}}\\right) = P\\left(Z &lt; \\frac{\\sqrt{10}}{5}\\right)\\]\nWe can use the pnorm function to compute normal probabilities in R. In this case:\n\npnorm(sqrt(10)/5)\n\n[1] 0.7364554\n\n\nAlternatively, we can provide the mean and and standard deviation (of \\(\\overline X\\), so \\(\\frac{\\sigma}{\\sqrt n}\\)) as additional arguments to pnorm:\n\npnorm(2, 1, 5/sqrt(10))\n\n[1] 0.7364554"
  },
  {
    "objectID": "jan22.html#t-distribution",
    "href": "jan22.html#t-distribution",
    "title": "Computing various probabiltiies in R",
    "section": "\\(t\\)-distribution",
    "text": "\\(t\\)-distribution\nNow let’s say that instead of a population variance of 25, we have a sample variance of 25. In this case, since we don’t know \\(\\sigma^2\\), we can use the \\(t\\)-distribution. In particular, we have \\(\\dfrac{\\overline X - \\mu}{S/\\sqrt n} \\sim t(n - 1)\\).\nSo if \\(T\\sim t(9)\\), we can say that\n\\[P(\\overline X &lt; 2) = P\\left(T &lt; \\frac{2 - 1}{5/\\sqrt{10}}\\right) = P\\left(T &lt; \\frac{\\sqrt{10}}{5}\\right)\\]\nWe can use the pt function in R to compute these probabilities. In general, if \\(T\\sim t(\\nu)\\) then we compute \\(P(T &lt; t)\\) using pt(t, ν). So in this case:\n\npt(sqrt(10)/5, 9)\n\n[1] 0.7285895"
  },
  {
    "objectID": "jan22.html#quantile-functions",
    "href": "jan22.html#quantile-functions",
    "title": "Computing various probabiltiies in R",
    "section": "Quantile functions",
    "text": "Quantile functions\nIf \\(X\\) is a continuous random variable with cumulative distribution function \\(F_X\\), then its quantile function is \\(Q_X = F^{-1}_X\\). In particular,\n\\[P(X\\leq x) = p \\iff Q_X(p) = x.\\]\nIn R, we can compute quantile functions using functions beginning with q. Let’s work all of our previous examples backwards.\n\nqchisq(0.6999858, 5, lower.tail = FALSE)\n\n[1] 3\n\n\n\nqnorm(0.7364554)\n\n[1] 0.6324556\n\nsqrt(10)/5\n\n[1] 0.6324555\n\n\n\nqt(0.7285895, 9)\n\n[1] 0.6324555"
  }
]